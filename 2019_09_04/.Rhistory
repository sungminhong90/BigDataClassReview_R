# 데이터 불러오기
df <- read.csv("newAdTracking.csv")
## 01 : 데이터 컬럼들을 확인해 보자
names(df)
## 02 : 데이터 속성을 확인해 보자
str(df)
## 03 : 데이터 행열을 확인해 보자
dim(df)
## 04 : 어떤 ip가 존재하는지 알아보자
head(df$ip)
## 04 : 어떤 ip가 존재하는지 알아보자
summary(df$ip)
## 04 : 어떤 ip가 존재하는지 알아보자
unique(df$ip)
## 04 : 어떤 ip가 존재하는지 알아보자
list(df$ip)
## 05 : 어떤 app가 존재하는지, 광고에 가장 많이 접속한 app를 알아보자
max(df$app)
## 04 : 어떤 ip가 존재하는지 알아보자
table(df$ip)
## 05 : 어떤 app가 존재하는지, 광고에 가장 많이 접속한 app를 알아보자
sort(table(df$app))
sort(table(df$app),decreasing = T)     3번 app
sort(table(df$app),decreasing = T)     ##05 3번 app
## 05 : 어떤 app가 존재하는지, 광고에 가장 많이 접속한 app를 알아보자
sort(table(df$app), decreasing = T)
## 06 : 어떤 device 존재하는지, 광고에 가장 많이 접속한 device를 알아보자
sort(df$device, decreasing = T)
sort(table(df$device),decreasing = T)  ##06 1번 device
## 06 : 어떤 device 존재하는지, 광고에 가장 많이 접속한 device를 알아보자
sort(table(df$device), decreasing = T)
## 07 : 어떤 os가 존재하는지, 광고에 가장 많이 접속한 os를 알아보자
sort(table(df$os), decreasing = T)
## 08 : 어떤 channel이 존재하는지, 광고에 가장 많이 접속한 channel을 알아보자
sort(table(df$channel), decreasing = T)
## 09 : 컬럼별로 결측치가 있는지 확인해 보자.
colSums (df)
## 09 : 컬럼별로 결측치가 있는지 확인해 보자.
colSums(df)
## 09 : 컬럼별로 결측치가 있는지 확인해 보자.
colSums(is.na(df))
# 데이터를 R 검색 경로에 추가하여 변수명으로 바로 접근할 수 있게 한다.
attach(df)
model <- glm(is_attributed ~ ip + app + device + os + channel, family = binomial, data = df)
# 예측하기 위한 새로운 데이터를 만들어 보자.
new_df <- data.frame( ip = c(2948, 2684, 2685, 2686)
, app = c(19 , 286, 21, 23)
, device = c(0, 1, 1, 2)
, os = c(13 , 25, 13, 13)
, channel = c(213, 256, 122, 459))
## 11 : new_df의 ip, app, channel 값을 통해 is_attributed(어플을 다운 받았는지) 값을 예측해 보자.
pred <- predict(model, newdata = new_df, type = "response")
pred
new_df <- new_df %>%
mutate(is_attributed = as.integer(pred > 0.5))
new_df <- new_df %>%
mutate(is_attributed = as.integer(pred > 0.5))
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
new_df <- new_df %>% mutate(is_attributed = as.integer(pred > 0.5))
new_df <- new_df %>%
mutate(is_attributed=as.integer(pred>0.5))
library(dplyr)
new_df <- new_df %>% mutate(is_attributed = as.integer(pred > 0.5))
## 12 : 예측된 결과값을 출력해 주세요.
new_df
## 타이타닉 머신러닝 해보기
install.packages("Amelia")
library(Amelia)
library(ggplot2)
##데이터를 불러와볼까요?
train <- read.csv("titanic_train",
stringsAsFactors = F,
na.strings = c("", "NA"))
##데이터를 불러와볼까요?
train <- read.csv("titanic_train.csv",
stringsAsFactors = F,
na.strings = c("", "NA"))
# 여기서 stringsAsFactors는 팩터형 말고 그냥 문자형 그대로 데이터로 가져온다!
# na.strings = c(~) 이거는 혹시 문자가 들어있는 행이 있을 때 factor로 인지되는 것을 방지하기 위해 NA/INT로 바꾸어 준다. 여기서는 "" & NA를 NA로 인식
test <- read.csv("titanic_test.csv",
stringsAsFactors = F,
na.strings = c("", "NA"))
sub <- read.csv("sample_submission.csv",
stringsAsFactors = F,
na.strings = c("", "NA"))
dim(train); dim(test); dim(sub)
names(train)
names(test)
names(sub)
str(train)
missmap(train)
missmap(test)
colSums(is.na(train))
colSums(is.na(test))
quantile(train$Age, na.r = T)
quantile(test$Age, na.r = T)
quantile(train$Age, na.rm = T)
quantile(test$Age, na.rm = T)
#age 결측치 처리
nrow(train[is.na(train$Age), ])
#=> 177개 처리해줘야 함
nrow(train[is.na(train$Age)])
train[is.na(train$Age), 'Age'] = median(train$Age, na.rm = T)
##test용 데이터 처리
nrow(test[is.na(test$Age), ])
test[is.na(test$Age), 'Age'] = median(test$Age, na.rm = T)
#결측치 처리되었나 확인해보기!
nrow(train[is.na(train$Age), ]); nrow(test[is.na(test$Age), ])
#정박항(Embarked) 결측치 처리
##다수의 값이 나온 것을 이걸로 판단
cnt_tr <- table(train$Embarked, useNA = 'always')
cnt_test <- table(test$Embarked, useNA = 'always')
cnt_tr; cnt_test
## 학습용 데이터를 처리해보쟈
nrow(train[is.na(train$Embared), ])
nrow(test[is.na(test$Embarked), ])
train [ is.na(train$Embarked),'Embarked' ] = 'S'
nrow(train[is.na(train$Embared), ])
#데이터 확인
colSums(is.na(train))
colSums(is.na(test))
test[is.na(test$Fare),'Fare'] = median(test$Fare, na.rm = T)
#데이터 확인
colSums(is.na(train))
colSums(is.na(test))
#데이터 모델을 만들어보쟈!
model <- glm(Survived~Pclass+Age+Sex,
family = binomial, data=train)
summary(model)
##제일 중요한 예측!!!!
pred <- predict(model, newdata = test, type ='response')
#type을 response로 지정하고 예측을 수행하면 0 ~ 1 사이의 확률을 구해준다.
pred[0:10]
pred <-as.integer(pred >0.5)
pred[0:10]
sub[, Survived] = pred
sub[, 'Survived'] = pred
sub[0:10]
sub[0:10, ]
getwd()
#파일로 저장해주기!
write.csv(sub, file = "firstsub.csv", row.names = F)
list.files(path=".", pattern = Null)
list.files(path=".")
#파일로 저장해주기!
write.csv(sub, file = "firstsub.csv", row.names = F)
## 타이타닉 머신러닝 해보기_ver2
library(dplyr)
library(caret)
install.packages('caret')
library(caret)
install.packages('rpart')
install.packages("rpart")
library(rpart)
library(caret)
install.packages('randomForest')
library(randomForest)
set.seed(1004)
#데이터 불러오기
train <- read.csv("titanic_train.csv",
stringsAsFactors = F,
na.strings = c("", "NA"))
test <- read.csv("titanic_test.csv",
stringsAsFactors = F,
na.strings = c("", "NA"))
sub <- read.csv("sample_submission.csv",
stringsAsFactors = F)
#데이터 전처리
## 학습용데이터, 테스트 데이터를 하나로 만들어서 처리해보쟈!
names(test)
test$Survived <- NA
all <-rbind(train, test)
dim(all)
colSums(is.na(all))
#범주형으로 변환해보쟈!
##성별/생존유무/등급
all$Sex <- as.factor(all$Sex)
all$Survived <- as.factor(all$Survived)
all$Pclass <-as.factor(all$Pclass)
str(all)
#파생변수 생성
##Pclass & Sex를 이용한 변수 생성
all$PclassSex[all$Pclass == '1' & all$Sex == 'male'] <- 'P1Male'
all$PclassSex[all$Pclass == '2' & all$Sex == 'male'] <- 'P2Male'
all$PclassSex[all$Pclass == '3' & all$Sex == 'male'] <- 'P3Male'
all$PclassSex[all$Pclass == '1' & all$Sex == 'female'] <- 'P1female'
all$PclassSex[all$Pclass == '2' & all$Sex == 'female'] <- 'P2female'
all$PclassSex[all$Pclass == '3' & all$Sex == 'female'] <- 'P3female'
all$PclassSex <- as.factor(all$PclassSex)
names(all); table(all$PclassSex)
#데이터 전처리
## 결측치 확인
all[is.na(all$Fare), ]
all[is.na(all$Embared),]
all[is.na(all$Embarked),]
all %>% group_by(PclassSex) %>%
summarise(n=n(),
mean_age = mean(Age, na.rm = T),
median_age = median(Age, na.rm= T))
table(all$PclassSex)
#결측치 처리하기
all %>%  group_by(PclassSex) %>%  summarise(n=n(), #빈도수
mean_age = mean(Age, na.rm=T),
median_age = median(Age, na.rm=T))
all[is.na(all$Fare), 'Fare'] = median(all$Fare, na.rm = T)
#결측치 처리
all[is.na(all$Embarked), 'Embarked'] = 'S'
all[is.na(all$Age)&all$PclassSex =='P1Female', 'Age'] =36
all[is.na(all$Age)&all$PclassSex =='P2Female', 'Age'] =28
all[is.na(all$Age)&all$PclassSex =='P3Female', 'Age'] =22
all[is.na(all$Age)&all$PclassSex =='P1Male', 'Age'] =42
all[is.na(all$Age)&all$PclassSex =='P2Male', 'Age'] =29
all[is.na(all$Age)&all$PclassSex =='P3Male', 'Age'] =25
#n=n()빈도수
all %>% group_by(PclassSex) %>%
summarise(n=n(),
mean_age = mean(Age, na.rm = T),
median_age = median(Age, na.rm= T))
all$Embarked <- as.factor(all$Embarked)
trainClean <- all[!is.na(all$Survived), ]
#train파일을 다시 잘라 옴
nrow(trainClean)
#학습용(모델학습, 모델평가)
##데이터 나누기
##7:3으로 usually 나눈다
#밑의 idx는 비복원 추출 (샘플링!!!!!!!!!!)
idx <-sample(1:nrow(trainClean),
size =nrow(trainClean)*0.7,
replace = F)
train_tr <- trainClean[idx,]
train_test <-trainCealn[-idx, ]
train_test <-trainClean[-idx, ]
#제출용
testClean <- all[is.na(all$Srvive), ]
nrow(testClean);
#데이터 모델 만들기
##로지스틱 회귀 모델
m <-glm(Survived~Pclass+Sex+Age+Embarked+PclassSex, family = binomial, data = train_tr)
summary(m)
#데이터 모델 학습 후 예측
pred <-predict(m, newdata=train_test, type = "response")
pred[0:10]
pred <- as.integer(pred >0.5)
pred[0:10]
#데이터 모델 학숩 후 예측, 모델 평가
actual <- train_test[, "Survived"]
xt = xtabs(~pred + actual)
xt
library(caret)
pred <- as.factor(pred)
actual <- as.factor(actual)
confusionMatrix(pred, actual)
install.packages("e1071")
library(e1071)
pred <- as.factor(pred)
actual <- as.factor(actual)
confusionMatrix(pred, actual)
str(train_tr)
library(randomForest)
m2 <- randomForest(Survived~Pclass+Sex+PclassSex+Age+Fare+Embarked, data = train_tr)
m2 <- randomForest(Survived~PClass+Sex+PclassSex+Age+Fare+Embarked, data = train_tr)
m2 <- randomForest(Survived~Pclass+Sex+Age+Embarked+PclassSex, data = train_tr)
length(pred)
m2 <- randomForest(Survived~Pclass+Sex+Age+Embarked+PclassSex, data = train_tr)
str(train_tr)
m2 <- randomForest(Survived~Pclass+Sex+Age+Embarked+PclassSex, data = train_tr)
m2 <- randomForest(Survived~+Sex+Age+Embarked+PclassSex, data = train_tr)
m2 <- randomForest(Survived~Sex+Age+Embarked+PclassSex, data = train_tr)
m2 <- randomForest(Survived~Pclass+Sex+Age+Embarked+PclassSex, data = train_tr, family= binomial)
train_tr <- trainClean[idx,]
m2 <- randomForest(Survived~Pclass+Sex+Age+Embarked+PclassSex, data = train_tr, family= binomial)
m2 <- randomForest(Survived~Pclass+Sex+Age+Embarked+PclassSex, data = train_tr)
